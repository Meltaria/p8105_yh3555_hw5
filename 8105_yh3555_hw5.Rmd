---
title: "8105_yh3555_hw5"
author: "Yuchen Hua"
date: "2022-11-13"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(readr)
library(ggplot2)
library(dbplyr)
library(p8105.datasets)
library(viridis)
library(rvest)
library(patchwork)
set.seed(1)
```

# Problem 1
```{r}
fulldata = 
    tibble(
    files = list.files("data/data1/"),
    path = str_c("data/data1/", files)
) %>%
  mutate(data = map(path, read.csv)) %>%
  unnest()
```
The data were imported and combined together by tibble functions. A dataframe was created to include all the files in the directory. Path to each file was created. The paths was mapped and the data was imported by "read_csv" function. The result of "map" was unnested in the end. 

```{r}
tidydf = fulldata %>%
  mutate( 
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>%
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>%
  mutate(week = as.numeric(week)) %>%
  select(group, subj = files, week, outcome)
```
The data had been wide rather than long, so it should be cleaned up via pivot_longer.


```{r}
tidydf %>%
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) +
  geom_point() +
  geom_path() +
  facet_grid(~group)
```
Plot was created to show individuao data, facted by group. 
 
#  Problem 2
```{r}
homicide = read.csv("./data/data2/homicide-data.csv")
```
There were 52179 cases recorded from 50 cities. There were total 12 variables put into observation. The victim's first and last names were recorded, together with their race, age and sex. 

__
```{r}
homicide = homicide %>%
  mutate(city_state = str_c(city, ",", state)) %>%
  mutate(city_state = as.factor(city_state))
```
City variable and state variable were combined via str_c() function, seperated by ",". For further manipulate convinience, city_state varibale is changed from character to factor. 
```{r}
total = homicide %>%
  group_by(city_state) %>%
  summarize(total_homicide = n()) %>% 
  arrange(desc(total_homicide))
```
Among all 51 cities, Chicago has the most number of homicide, to be 5535 and Philadelphi has the second rank, with 3037 homicide. 

```{r}
unsolved = homicide %>%
  group_by(city_state, disposition) %>% 
  filter(disposition %in% c("Closed without arrest", "Open/No arrest")) %>% 
  summarize(case = n()) %>%
  pivot_wider(names_from = disposition, values_from = case) %>%
  janitor::clean_names() %>%
  replace_na(list(closed_without_arrest = 0, open_no_arrest = 0)) %>%
  mutate(total_unsolved = sum(closed_without_arrest, open_no_arrest)) %>% 
  arrange(desc(total_unsolved))
```
By grouping the city_state with disposition, "closed without arrest" and "Open/No arrest" were filtered. These two types of cases underwent pivot_wider function to be variables so that the number of cases in each city_state can be found via summarize(). The total number of unsolved cases is the the sum of that of Closed without arrest and Open/No arrest. The na is replaced with 0. 
In this tibble, we can find that Chicago, IL still have the most total unsolved cases, to be 4073 cases, with 3686 "Open/No arrest". The second one, Baltimore, MD, have 1825 unsolved cases, with 1673 "Open/No arrest.

```{r}
homicide_bycase = homicide %>%
  group_by(city_state, disposition) %>% 
  summarize(case = n()) %>%
  pivot_wider(names_from = disposition, values_from = case) %>% 
  janitor::clean_names() %>%
  replace_na(list(closed_by_arrest =0, closed_without_arrest = 0, open_no_arrest = 0)) %>%
  mutate(total_unsolved = sum(closed_without_arrest, open_no_arrest),
         total_case = sum(closed_without_arrest, open_no_arrest,closed_by_arrest)) %>% 
  select(city_state, total_unsolved, total_case)
homicide_bycase
```
The tibble was rearranged by mutate the cases into total_unsolved and total. To find the proportion, only city_state, total_unsolved and total variable were requried. Thus, only these 3 variables were selected. 


```{r}
propt = function(x,n) {
  prop.test(x,n) %>%
    broom::tidy() %>%
    select(estimate, conf.low, conf.high)
}
```
A function of prop.test function was created in order to analyze large number of variables. 

```{r}
baltimore = homicide_bycase %>%
  filter(city_state == c("Baltimore,MD"))
prop_bal = baltimore %>%
  mutate(bal_prop = map2(.x = total_unsolved, .y = total_case, .f= ~propt(x = .x, n = .y))) %>%
  unnest(bal_prop) %>%
  select(-total_unsolved, -total_case)
prop_bal
```
Baltimore's dataset was pulled from the homicide_bycase. The propr function, which had been created before, was used to estimated the proportion. map2 function was used for check the variable and funciton used in this test. From the result, the estimated proportion was found to be 0.646, with a CI (0.628, 0.663). 

```{r}
prop_city = homicide_bycase %>%
  mutate(city_prop = map2(.x = total_unsolved, .y = total_case, .f= ~propt(x = .x, n = .y))) %>%
  unnest(city_prop) %>% 
  select(-total_unsolved, -total_case)
prop_city
```
Similar process was applied to the whole dataset. Each city's estimate propotion and confidence interval was calculated. 


```{r}
plot_homicide = prop_city %>%
  ggplot(aes(x=fct_reorder(city_state, -estimate), y=estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax= conf.high)) + 
  labs(x = "City, State", y = "Proportion", title = "Proportion of unsolved homicide") + 
  theme(axis.text.x = element_text(angle = 90))
plot_homicide
```
A scatter plot was created via ggplot function. fct_reorder() was used to order the city by estimated proportion. Error bar was make for every point, based on the confidence interval. For a better reading, the x-axis texts were rotated.   


## Problem 3 ##
__Create the function__
```{r}
ttest = function(n = 30, mean, sd=5){
  data = rnorm(n=30, mean, sd=5)
  results = t.test(data, conf.level = 0.95)
  
  results %>%
    broom::tidy() %>% 
    select(estimate, p.value)
}
```
A ttest function was created with a fixed sample size of 30 and standared deviation of 5. The mean can be changed based on input. 

Let's run the mean=0 to generate 5000 datasets. 
```{r, cache=TRUE}
mean_0 = rerun(5000, ttest(mean=0)) %>%
  bind_rows
```
rerun() function was used for this simulation. The created ttest was applied for mean=0. The dataset was obtained by binding the rows. 


Let's repeat mean = {1, 2, 3, 4, 5, 6}
```{r, cache=TRUE}
mean_1_6 = 
  tibble(mean = c(1, 2, 3, 4, 5, 6)) %>%
  mutate(
    output = map(.x =mean, ~rerun(5000, ttest(mean = .x))),
    estimate = map(output, bind_rows) ) %>%
  select(-output) %>%
  unnest(estimate)
```
The dataset with mean from 1 to 6 was created with map() function. 

```{r}
prop_nullreject = mean_1_6 %>%
  mutate(
    reject = ifelse(p.value < 0.05, 1, 0)
  ) %>%
  group_by(mean) %>%
  summarize(n_reject = sum(reject),
            prop_reject = n_reject / 5000) %>%
  ggplot(aes(x = mean, y = prop_reject)) + geom_point() + labs(title = "Proportion of nll rejected", x = "True Mean", y= "Proportion of reject")
prop_nullreject
```
A plot of times of null rejected vs True mean was created. From the plot, we can found that, with a higher True mean, the portion of rejected times increased and approached to 1 finally. 

```{r}
plot_all_mean = 
  mean_1_6 %>%
  group_by(mean) %>%
  summarize(ave_esti = mean(estimate)) %>%
  ggplot(aes(x = mean, y =ave_esti)) + geom_point() +
  labs(title ="Average estimate mean vs True mean",
       x = "True mean", y = "Average estimate mean")
plot_all_mean
```

```{r}
plot_reject_mean =
  mean_1_6 %>%
  filter(p.value < 0.05) %>%
  group_by(mean) %>%
  summarize(ave_esti = mean(estimate)) %>%
  ggplot(aes(x =mean, y = ave_esti)) + geom_point() +
  labs(title ="Average rejected estimate mean vs True mean",
       x = "True mean", y = "Average estimate rejected mean")
plot_reject_mean
```

```{r}
plot_full= plot_all_mean + plot_reject_mean
plot_full
```

From the two-panel plot, the average of mean from the test where the null was rejected was not equal to the true mean. These values were rejected, indicating they may be greatly different from the true mean. 


